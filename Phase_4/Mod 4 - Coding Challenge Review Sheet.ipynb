{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "### Notes\n",
    "* Unsupervised Learning Technique\n",
    "* dimenstionality reduction technique\n",
    "\n",
    "Unsupervised Learning vs Supervised Learning\n",
    "* Supervised is trying to answer a very specific question, it is looking to find the traits of a scecific value of the target variable (clear task)\n",
    "    * data = fruit basket, target variable = type of fruit, supervised is trying to build a model that identifies apples)\n",
    "* Unsupervised is just trying to build a model that makes clusters of the data based on their feature values (no clear task)\n",
    "    * data = fruit basket, target variable = type of fruit, unsupervised is trying to build a model that makes clsuters of fruit based on the characterstics of each piece\n",
    "\n",
    "The Curse of Dimensionality\n",
    "* the more feautures you have the larget the space between points (for things like clustering) making the computation almost impossible\n",
    "* more feautres = more info = larder the n-dimentional space becomes\n",
    "\n",
    "**What is PCA used for?**\n",
    "* identify key features (dimensionality reduction)\n",
    "* improve regression and classification algorithms\n",
    "* helps create visualizations\n",
    "\n",
    "\n",
    "**How does it decrease dimensionality?**\n",
    "* It decreases dimensionality by finding feature that can explain a high % of the variance so additional feautres gain little explain variance\n",
    "\n",
    "**What are some of the attributes of PC's?**\n",
    "\n",
    "\n",
    "**How to implement in SKLEARN, and how can we specifiy number of components?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "# split data into X and Y\n",
    "features = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "X = df[features].values\n",
    "y = df['Target'].values\n",
    "\n",
    "# Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the features\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Preview X\n",
    "pd.DataFrame(data=X, columns=features).head()\n",
    "\n",
    "# Import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Instantiate PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit PCA\n",
    "principalComponents = pca.fit_transform(X)\n",
    "\n",
    "# Create a new dataset from principal components \n",
    "df = pd.DataFrame(data = principalComponents, \n",
    "                  columns = ['PC1', 'PC2'])\n",
    "\n",
    "target = pd.Series(iris['target'], name='target')\n",
    "\n",
    "result_df = pd.concat([df, target], axis=1)\n",
    "result_df.head(5)\n",
    "\n",
    "print('Variance of each component:', pca.explained_variance_ratio_)\n",
    "print('\\n Total Variance Explained:', round(sum(list(pca.explained_variance_ratio_))*100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "### Notes\n",
    "Clustering\n",
    "* group objects into similar classes\n",
    "    * intra-class similarity is high (similarity amongst members of the same group is high)\n",
    "    * inter-class similarity is low (similarity of different groups is low)\n",
    "* agglomertative hierarchical - starts with n clusters and joins the most similar clusters until 1 single cluster\n",
    "* divisive hierarchical - goes from 1 to n clusters\n",
    "* nonhierarchical - chooses k initial clusters and reassigns observations until no improvement can be obtained\n",
    "\n",
    "K-means clustering\n",
    "* trying to find k cluster centeres as the mean of the data points that belong to these clusters (drawback is k is determined before running)\n",
    "* here are the steps\n",
    "    * Select  initial seeds\n",
    "    * Assign each observation to the cluster to which it is \"closest\"\n",
    "    * Recompute the cluster centroids\n",
    "    * Reassign the observations to one of the clusters according to some rule\n",
    "    * Stop if there is no reallocation\n",
    "* assumptions for kmeans\n",
    "    * a cluster centroid is just the mean of all points\n",
    "    * after centroid is recalculated all points are then reassigned\n",
    "    \n",
    "\n",
    "**How to implement kmeans in sklearn?**\n",
    "\n",
    "**What is the step-by-step process of the algorithm?**\n",
    "* above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k_means = KMeans(n_clusters=6, # number of clusters\n",
    "                init='k-means++', # how to choose starting points for clusters\n",
    "                algorithm='%autocallto')\n",
    "k_means.fit(X)\n",
    "predicted_clusters = k_means.predict(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP\n",
    "### Notes\n",
    "* Bag of words - this is a container that has every word that occurs in any text in the corpus along with the number of times it appeared\n",
    "* Stemming - removing the end of words to get just the stem of the word (makes cats -> cat)\n",
    "* Lemming - a more advanced method of finding the real root word\n",
    "\n",
    "Example\n",
    "| Word     | Stem  | Lemma |\n",
    "|----------|-------|-------|\n",
    "| Studies  | Studi | Study |\n",
    "| Studying | Study | Study |\n",
    "\n",
    "* N-grams and Mutual Information Scpre\n",
    "    * a bigram (ngram of 2) is a pair of words that appear next to eachother in the document\n",
    "    * Pointwise mutual information gives us the liklyhood that a word is paired with another ('San' and 'Francisco' would get a high score becayse when you see San, Fransisco is almost always after it)\n",
    "\n",
    "**What is tf-idf?**\n",
    "* The idea that a rare word contains more information about the document than wrods that are used many times throughout all documents\n",
    "* term-frequency(t): $$\\large Term\\ Frequency(t) = \\frac{number\\ of\\ times\\ t\\ appears\\ in\\ a\\ document} {total\\ number\\ of\\ terms\\ in\\ the\\ document} $$\n",
    "* inverse document frequency: $$\\large IDF(t) = log_e(\\frac{Total\\ Number\\ of\\ Documents}{Number\\ of\\ Documents\\ with\\ t\\ in\\ it})$$\n",
    "\n",
    "**How to interpret the value of a token that has high tf-idf?**\n",
    "The higher the score the more relevant that word is in that particular document.\n",
    "\n",
    "**How to properly apply `tfidfvectorizer()` method in sklearn to a corpus and apply machine learning algorithms to the resulting features?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF implementation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tf_idf_data_train = vectorizer.fit_transform(data)\n",
    "# use transform bc the vectorizer has already been fit to the training data\n",
    "tf_idf_data_test = vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "# looking at how sparse the matrix is\n",
    "non_zero_cols = tf_idf_data_train.nnz / float(tf_idf_data_train.shape[0])\n",
    "print(\"Average Number of Non-Zero Elements in Vectorized Articles: {}\".format(non_zero_cols))\n",
    "\n",
    "percent_sparse = 1 - (non_zero_cols / float(tf_idf_data_train.shape[1]))\n",
    "print('Percentage of columns containing 0: {}'.format(percent_sparse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series\n",
    "#### Notes\n",
    "* turn the date column into a datetime, then set it as index\n",
    "    * `temp_data['Date'] = pd.to_datetime(temp_data['Date'], format='%d/%m/%y')`\n",
    "    * `temp_data.set_index('Date', inplace=True)`\n",
    "\n",
    "\n",
    "Getting Rid of Trends\n",
    "* taking log or square root can help\n",
    "* subtracting the rolling mean - this will level out the graph even more\n",
    "* subtracting the weighted rolling mean - more recent values are given a higher weight\n",
    "* **differencing** - we take the difference of an observation at a particular time instant with that at the previous instant (i.e. a so-called 1-period \"lag\")\n",
    "\n",
    "**How to visualize a time series data?**\n",
    "* Normal df.plot() works well but a dot plot can show outliers easily\n",
    "* grouping by year can help show yearly trends\n",
    "* many other types in the visualizing time series data lab\n",
    "\n",
    "**Can you interpret whether stationarity or trend exist in a visualization?**\n",
    "* Stationarity - it is stationary if statistical properties like mean, variance, etc. remain constant over time\n",
    "* Linear Trend\n",
    "    * upward trend sees the graph increasing as time goes on\n",
    "    * downward trend being the opposite\n",
    "* exponential trend\n",
    "* periodic being it goes up and down depending on the time (temps go up in summer down in winter)\n",
    "* Increasing Variance means the up and down are more drastic as time goes on\n",
    "\n",
    "**How to calculating rolling mean given a specified window?**\n",
    "* below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of rolling trend\n",
    "roll_mean = ts.rolling(window=8, center=False).mean()\n",
    "roll_std = ts.rolling(window=8, center=False).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get differenced values\n",
    "diff = shampoo.diff().rename(index=str, columns={\"Sales of shampoo over a three year period\": \"Differenced Observations\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform dickey fuller to see if our data is stationary\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "test = adfuller(diff['Differenced Observations'][1:-1])\n",
    "dfoutput = pd.Series(test[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "print(dfoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACF/PACF to determine which terms in include (MA or AR or Both?)\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "#plot autocorrelation for each lag (alpha is confidence   interval)\n",
    "plot_acf(shampoo[:-1], alpha=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "plot_pacf(shampoo[:-1], alpha=.05, lags=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
