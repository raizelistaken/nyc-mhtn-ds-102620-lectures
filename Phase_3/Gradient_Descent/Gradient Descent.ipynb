{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "_Dec 10, 2020_\n",
    "\n",
    "Agenda today:\n",
    "- Gradient Descent overview with OLS\n",
    "- The Cost Curve and Cost functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinary Least Squares is a method for estimating the unknown parameters in a linear regression model. OLS tries to find the parameters that minimizes the Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the parameters to minimize the cost function using linear algebra : \n",
    "\n",
    "Pros: \n",
    "    - Closed form solution - gives us the \"right\" answer\n",
    "Cons: \n",
    "    - Computationally expensive - lots of complex math "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent offers a more flexible way of \"learning\" our parameters as we go, instead of calculating them in one go. In order to understand gradient descent we need to have a good understanding of the cost curve: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cost Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is cost?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align:center;font-size:20px'>$MSE = \\frac{1}{n}\\sum_{i=1}^n (Y_i - \\hat Y_i)^2 $</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1, 1, 2, 3, 4, 3, 4, 6, 4]\n",
    "y = [2, 1, 0.5, 1, 3, 3, 2, 5, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align:center;font-size:20px'>$ Y = \\beta_0  + \\beta_1 x $</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate some random lines using the above formula\n",
    "beta_0 = 0\n",
    "beta_1 = [.25, .5, .75, .8, 1,]\n",
    "#beta_1 = list(np.arange(0,5,0.1))\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "mses = []\n",
    "for t in beta_1:\n",
    "    line = beta_0 + (np.array(x)*t)\n",
    "    mse = round(mean_squared_error(y, line),3)\n",
    "    mses.append(mse)\n",
    "    ax.plot(x, line, label=f'{mse} {t}')\n",
    "ax.scatter(x,y)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Cost Curve\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot(beta_1, mses)\n",
    "ax.set_title('Cost Curve')\n",
    "ax.set_xlabel('bheta 1')\n",
    "ax.set_ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the steps we take to get to the \"bottom\" of the curve? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/ralph.gif'/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is an <b>algorithm</b> used to find the lowest point of a function. It is a process that helps us change our parameters (coefficients) until we get the optimal parameters of our function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align:center;font-size:20px'>$ \\theta_j := \\theta_j - \\alpha * \\frac{\\partial J(\\theta)}{\\partial\\theta_i} $</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we go about changing our parameters? \n",
    "\n",
    "The Steps:\n",
    "    - Find the gradient (how steep is the curve) \n",
    "    - Multiply Learning Rate * Gradient - to calculate the amount you want to change \n",
    "    - Subtract above value from current parameter to create new parameter estimate|\n",
    "    - Repeat until gradient is ~ 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/gradient_desc.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little calculus : What does the derivative of a function tell us? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align:center;font-size:20px'>$\\frac{dy}{dx}$</p>\n",
    "<p style='text-align:center;font-size:20px'>What does this translate to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a derivative tells us how much our output will change if we change a parameter just a little bit than we can use this information to update our parameter to be closer to the bottom. When our derivative is large that means our parameter is resulting in a high cost and we want to make a significant change. Conversely, if our derivative is small we don't need to make as much of a change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealting with Multiple Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align:center;font-size:20px'>$h_\\theta(x) = \\theta_0 + \\theta_1 x $</p>\n",
    "<p style='text-align:center;font-size:20px'>$MSE = \\frac{1}{2n}\\sum_{i=1}^n (Y_i - \\hat Y)^2 $</p>\n",
    "<p style='text-align:center;font-size:20px'>$MSE = \\frac{1}{2n}\\sum_{i=1}^n (Y_i - \\theta_0 + \\theta_1 x_i)^2 $</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with multiple parameters we need to use partial derivatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style='width:300px' src='images/partial.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each partial derivative tells us how much a small change in the parameter will affect the cost. Same as before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know how much we need to change our parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate helps us \"soften\" the amount of change we apply to a parameter? Why might we want to do this? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://developers.google.com/machine-learning/crash-course/fitter/graph?source=post_page---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate is usually denoted as $ \\alpha $ and chosen before you start the \"learning\" process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a learning rate and a gradient, we can update our parameter and see if we can improve our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align:center;font-size:20px'>$ \\theta_j := \\theta_j - \\alpha * \\frac{\\partial J(\\theta)}{\\partial\\theta_i} $</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep repeating the steps above until your parameters stop changing aka there is no more gradient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems With Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style='width:300px' src='images/local_min_meme.jpg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local minima occur when there are multiple minimum points in your cost function :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style='width:400px' src='images/minima.jpg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The speed of gradient descent is affected by the scale of the numbers you are dealing with. Larger numbers = more surface to traverse to get to a minimum. Scaling can help speed up the process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ols = linear_model.LinearRegression()\n",
    "ols.fit(x.reshape(-1,1), y)\n",
    "print(f'theta_0: {ols.intercept_}')\n",
    "print(f'theta_1: {ols.coef_[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gd = linear_model.SGDRegressor(alpha=.0001, max_iter=100000000, tol=None\n",
    "                              )\n",
    "gd.fit(x.reshape(-1,1),y)\n",
    "print(f'theta_0: {gd.intercept_[0]}')\n",
    "print(f'theta_1: {gd.coef_[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why use gradient descent? / Pros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Uses For Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Regression\n",
    "- Decision Trees (Boosted Trees) \n",
    "- Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Understanding the Mathematics Behind Gradient Descent](https://towardsdatascience.com/understanding-the-mathematics-behind-gradient-descent-dde5dc9be06e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
