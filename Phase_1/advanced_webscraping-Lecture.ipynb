{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Off\n",
    "\n",
    "With a partner, answer the following question:\n",
    "\n",
    "Is it legal to scrape data from websites?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Webscraping: How to make sure you don't get blocked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aims:\n",
    "\n",
    "- Write scripts that can handle errors and minimize the likelihood of your IP address getting blocked.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "- Talk about the legality of scraping\n",
    "- Practice scraping\n",
    "- Look at ways to programmatically avoid getting banned\n",
    "- Set up the selenium webdriver\n",
    "- Learn how to use Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check 200 status code\n",
    "It is always good to check the HTTP status code earlier and proceed accordingly.\n",
    "\n",
    "This is good:\n",
    "\n",
    "~~~\n",
    "if response.status_code == 200:\n",
    "   #Proceed further\n",
    "~~~\n",
    "\n",
    "This is better:\n",
    "\n",
    "~~~~\n",
    "if response.status_code != 200:\n",
    "  return False\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "page = requests.get(\"http://dataquestio.github.io/web-scraping-pages/simple.html\")\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://www.rottentomatoes.com/m/toy_story')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BeautifulSoup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-1012897ceec4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'BeautifulSoup' is not defined"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(r.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-47b87e7cdae3>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-47b87e7cdae3>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    soup.findAll('div', class = 'panel-body')\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "soup.findAll('div', class = 'panel-body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'urls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-eb0d89af090a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# include code to do status check\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'urls' is not defined"
     ]
    }
   ],
   "source": [
    "for url in urls:\n",
    "    page = requests.get(url)\n",
    "    # include code to do status check\n",
    "    if page.status_code != 200:\n",
    "        print( page.status_code)\n",
    "    \n",
    "    # more code to process the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Never Trust HTML\n",
    "\n",
    "Especially if you can’t control it. Web scraping depends on HTML DOM, a simple change in element or class name could break your entire script. The best way to deal with it is to check if it returns `None`.\n",
    "\n",
    "~~~\n",
    "page_count = soup.select('.pager-pages > li > a')\n",
    "if page_count:\n",
    " #do your stuff\n",
    "else:\n",
    " # ALERT!! Send notification to Admin\n",
    "~~~\n",
    "\n",
    "Here I am checking whether the CSS selector returned something legitimate, if yes then proceed further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in urls:\n",
    "    page = requests.get(url)\n",
    "    # include code to do status check\n",
    "    if page.status_code != 200:\n",
    "        print( page.status_code)\n",
    "        continue\n",
    "    # more code to process the results\n",
    "    #imagine we have gotten the contents of the page in the soup variable\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    items = soup.select(' .specific_class')\n",
    "    if items:\n",
    "        #continue processing the data\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Data is coming back blank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 .  Set headers\n",
    "\n",
    "`requests` does not force you to use request headers while sending requests, but there are few smart websites that do not let you to get read anything important unless certain headers are not set in it. Once I faced the situation that the HTML I was seeing in browser was different than what I was getting via my script, kind of like magic huh. So, it is always good to make your requests as legitimate as you can. The least you should do is to set a User-Agent.\n",
    "\n",
    "~~~\n",
    "headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}\n",
    "\n",
    "response = requests.get(url, headers=headers, timeout=5)\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can spoof the web browser by making it look like we are coming from an certain URL by using headers\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}\n",
    "\n",
    "for url in urls:\n",
    "    page = requests.get(url, headers = headers)\n",
    "    # include code to do status check\n",
    "    if page.status_code != 200:\n",
    "        print(page.status_code)\n",
    "    \n",
    "    #imagine we have gotten the contents of the page in the soup variable\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    items = soup.select(' .specific_class')\n",
    "    \n",
    "    #check to make sure we have items of that class\n",
    "    if items:\n",
    "        #continue processing the data\n",
    "    else:\n",
    "        print(\"Data is coming back blank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set timeout\n",
    "\n",
    "One of the issues with `requests` is that, if you don’t mention **timeout**, it will continue waiting for a response indefinitely. If your request is never fulfilled, it will leave your script haning there waiting for a response.  \n",
    "\n",
    "To set the request’s timeout, use the timeout parameter. timeout can be an integer or float representing the number of seconds to wait on a response before timing out:\n",
    "\n",
    "~~~\n",
    "response = requests.get(url, headers=headers, timeout=5)\n",
    "~~~\n",
    "\n",
    "\n",
    "You can also pass a tuple to timeout with the first element being a connect timeout (the time it allows for the client to establish a connection to the server), and the second being a read timeout (the time it will wait on a response once your client has established a connection):\n",
    "\n",
    "~~~ \n",
    "requests.get('https://api.github.com', timeout=(2, 5))\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the timeout tuple, first number is how to wait for a connection and second is how long to wait for a response\n",
    "\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}\n",
    "\n",
    "for url in urls:\n",
    "    page = requests.get(url, headers = headers, timeout=5)\n",
    "    # include code to do status check\n",
    "    if page.status_code != 200:\n",
    "        print(page.status_code)\n",
    "    \n",
    "    #imagine we have gotten the contents of the page in the soup variable\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    items = soup.select(' .specific_class')\n",
    "    \n",
    "    #check to make sure we have items of that class\n",
    "    if items:\n",
    "        #continue processing the data\n",
    "    else:\n",
    "        print(\"Data is coming back blank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exception handling\n",
    "\n",
    "It is always good to implement exception handling. It does not only help to avoid unexpected exit of script but can also help to log errors and info notification. When using Python requests I prefer to catch exceptions like this:\n",
    "\n",
    "~~~\n",
    "try:\n",
    "    # your logic is here\n",
    "\n",
    "except requests.ConnectionError as e:\n",
    "    print(\"OOPS!! Connection Error. Make sure you are connected to Internet. Technical Details given below.\\n\")\n",
    "    print(str(e))\n",
    "except requests.Timeout as e:\n",
    "    print(\"OOPS!! Timeout Error\")\n",
    "    print(str(e))\n",
    "except requests.RequestException as e:\n",
    "    print(\"OOPS!! General Error\")\n",
    "    print(str(e))\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Someone closed the program\") \n",
    "~~~\n",
    "\n",
    "Check the very last one. This one tells the program that if someone wants to terminate program by using Ctrl+C then it wrap things up first and then exist. This situation is good if you are storing information in file and wants to dump all at the time of exit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}\n",
    "\n",
    "for url in urls:\n",
    "    try:\n",
    "        page = requests.get(url, headers = headers, timeout=5)\n",
    "    # include code to do status check\n",
    "        if page.status_code != 200:\n",
    "            print(page.status_code)\n",
    "    except requests.ConnectionError as e:\n",
    "        print(\"OOPS!! Connection Error. Make sure you are connected to Internet. Technical Details given below.\\n\")\n",
    "        print(str(e))\n",
    "    except requests.Timeout as e:\n",
    "        print(\"OOPS!! Timeout Error\")\n",
    "        print(str(e))\n",
    "    except requests.RequestException as e:\n",
    "        print(\"OOPS!! General Error\")\n",
    "        print(str(e))\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Someone closed the program\") \n",
    "        \n",
    "        \n",
    "        \n",
    "    #imagine we have gotten the contents of the page in the soup variable\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    items = soup.select(' .specific_class')\n",
    "    \n",
    "    #check to make sure we have items of that class\n",
    "    if items:\n",
    "        #continue processing the data\n",
    "    else:\n",
    "        print(\"Data is coming back blank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is starting to get long and hard to read. So let's start to modularize it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these exceptions would most likely require the continue operator to go to the next iteration in the for loop\n",
    "def get_page(url):\n",
    "    try:\n",
    "        page = requests.get(url, headers = headers, timeout=5)\n",
    "    # include code to do status check\n",
    "        if page.status_code != 200:\n",
    "            print(page.status_code)\n",
    "    except requests.ConnectionError as e:\n",
    "        print(\"OOPS!! Connection Error. Make sure you are connected to Internet. Technical Details given below.\\n\")\n",
    "        print(str(e))\n",
    "    except requests.Timeout as e:\n",
    "        print(\"OOPS!! Timeout Error\")\n",
    "        print(str(e))\n",
    "    except requests.RequestException as e:\n",
    "        print(\"OOPS!! General Error\")\n",
    "        print(str(e))\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Someone closed the program\") \n",
    "        \n",
    "        \n",
    "    return page\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can replace a chunk of our code with this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}\n",
    "\n",
    "for url in urls:\n",
    "    #use our new function to process each url\n",
    "    page = get_page(url)\n",
    "        \n",
    "        \n",
    "    #imagine we have gotten the contents of the page in the soup variable\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    items = soup.select(' .specific_class')\n",
    "    \n",
    "    #check to make sure we have items of that class\n",
    "    if items:\n",
    "        #continue processing the data\n",
    "    else:\n",
    "        print(\"Data is coming back blank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Regulate your request pace\n",
    "\n",
    "Many websites have a limit on how many times you can ping a website within a minute/hour/day. YOu want to be aware of that and change your script in order to account for that.\n",
    "\n",
    "One example is using the `sleep()` function that is a part of the time package.  This can pause your script for a set amount of time.\n",
    "\n",
    "~~~\n",
    "import time\n",
    " \n",
    " \n",
    "## Star loop ##\n",
    "for url in urls:\n",
    "\n",
    "    # try to make resquest here.\n",
    "    \n",
    " \n",
    "    #### Delay for 1 seconds ####\n",
    "    time.sleep(1)\n",
    "        \n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time.sleep also takes in floats, it allows the program to pause for the time allotted every time its called\n",
    "import time\n",
    " \n",
    " \n",
    "## Start loop ##\n",
    "for url in urls:\n",
    "    print(\"Current date & time \" + time.strftime(\"%c\"))\n",
    "\n",
    "    #use our new function to process each url\n",
    "    page = get_page(url)\n",
    "             \n",
    "    #imagine we have gotten the contents of the page in the soup variable\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    items = soup.select(' .specific_class')\n",
    "    \n",
    "    #check to make sure we have items of that class\n",
    "    if items:\n",
    "        #continue processing the data\n",
    "    else:\n",
    "        print(\"Data is coming back blank\")\n",
    "    \n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Save as you go\n",
    "\n",
    "You might run into an issue halfway through your scrape and your script breaks. So you want to make sure you are saving your data as you go.  \n",
    "\n",
    "~~~ \n",
    "import csv\n",
    "...\n",
    "with open(\"~/Desktop/output.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # collected_items = [\n",
    "    #   [\"Product #1\", \"10\", \"http://example.com/product-1\"],\n",
    "    #   [\"Product #2\", \"25\", \"http://example.com/product-2\"],\n",
    "    #   ...\n",
    "    # ]\n",
    "\n",
    "    for item_property_list in collected_items:\n",
    "        writer.writerow(item_property_list)\n",
    "~~~\n",
    "~~~\n",
    "import csv\n",
    "...\n",
    "field_names = [\"Product Name\", \"Price\", \"Detail URL\"]\n",
    "with open(\"~/Desktop/output.csv\", \"w\") as f:\n",
    "    writer = csv.DictWriter(f, field_names)\n",
    "\n",
    "    # collected_items = [\n",
    "    #   {\n",
    "    #       \"Product Name\": \"Product #1\",\n",
    "    #       \"Price\": \"10\",\n",
    "    #       \"Detail URL\": \"http://example.com/product-1\"\n",
    "    #   },\n",
    "    #   ...\n",
    "    # ]\n",
    "\n",
    "    # Write a header row\n",
    "    writer.writerow({x: x for x in field_names})\n",
    "\n",
    "    for item_property_dict in collected_items:\n",
    "        writer.writerow(item_property_dict)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}\n",
    "\n",
    "for url in urls:\n",
    "    print(\"Current date & time \" + time.strftime(\"%c\"))\n",
    "\n",
    "    #use our new function to process each url\n",
    "    page = get_page(url)\n",
    "             \n",
    "    #imagine we have gotten the contents of the page in the soup variable\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    items = soup.select(' .specific_class')\n",
    "    \n",
    "    #check to make sure we have items of that class\n",
    "    if items:\n",
    "        #continue processing the data\n",
    "    else:\n",
    "        print(\"Data is coming back blank\")\n",
    "    \n",
    "    #Saving your data as you go\n",
    "    \n",
    "    # Option 1: write the line of data to a csv files\n",
    "    with open(\"~/Desktop/output.csv\", \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "\n",
    "    for item in items:\n",
    "        writer.writerow(item)\n",
    "        \n",
    "    # Option 2: Inseting the data into a DB\n",
    "    # This code uses a theoretical module, SQL,\n",
    "    # The functions below are examples and will not run. \n",
    "    import sql_helpers as sql\n",
    "    \n",
    "    sql.create_connection()\n",
    "    for  item in items:\n",
    "        item = data\n",
    "        query = \"INSERT INTO table_name VALUES (%s,%s,%s,%s)\"\n",
    "        sql.insert_data(db, query, data )\n",
    "        \n",
    "    #Taking a one second pause to help slow down your requests \n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Resources \n",
    "- [More advanced issues](https://blog.hartleybrody.com/web-scraping-cheat-sheet/)\n",
    "- [Request Advanced Usage](http://docs.python-requests.org/en/master/user/advanced/#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping with Python often requires no more than the use of the Beautiful Soup module to reach the goal. Beautiful Soup is a popular Python library that makes web scraping by traversing the DOM (document object model) easier to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied: Scraping Amazon's Best Sellers list:\n",
    "\n",
    "\n",
    "Amazon keeps track of the best sellers for 41 different categories of products. We want to grab that data from Amazon so that we can keep track of which products are on that list and stock our mom and pop store with them.  \n",
    "\n",
    "\n",
    "Deliverable: a file that contains all of the products on Amazon's best seller list. \n",
    "\n",
    "```[{'name': 'A top selling product',\n",
    "'url': http://the_url_to_the_product.com},\n",
    "{'name': 'A top selling product',\n",
    "'url': http://the_url_to_the_product.com}]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we start by grabbing the page where all of the best sellers list are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "url=\"https://www.amazon.com/Best-Sellers/zgbs\"\n",
    "\n",
    "#let's use the function we already created\n",
    "page = requests.get(url,headers=headers, timeout=5)\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BS(page.content, 'lxml')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this page, we want to find the urls of all the other pages to scrape those.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = soup.find(id='zg_browseRoot').findAll('li')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of all best seller urls\n",
    "urls = [url.find('a')['href'] for url in urls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a url/products that you want to investigate and lets build our script to parse one page.  then we can apply it to all of the pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=urls[3]\n",
    "\n",
    "apps = requests.get(url)\n",
    "apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_soup = BS(apps.content, 'lxml')\n",
    "print(app_soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the actual webpage to determine the data you want and the corresponding elements you want to parse out. Then use that element tag or class to pull those elements out of the page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "names = app_soup.find_all(class_='p13n-sc-truncate')\n",
    "price = app_soup.find_all(class_='p13n-sc-price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [x.text.strip() for x in names]\n",
    "price = [y.text for y in price]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo = list(zip(names,price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you can access all the data you need, let's put this into a loop so that we can proccess all of the products and create one list with all of the data.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "lst = []\n",
    "for url in urls:\n",
    "    res = requests.get(url)\n",
    "    soup = BS(res.content, 'html.parser')\n",
    "    names = soup.find_all(class_='p13n-sc-truncate')\n",
    "    price = soup.find_all(class_='p13n-sc-price')\n",
    "    names = [x.text.strip() for x in names]\n",
    "    price = [y.text for y in price]\n",
    "    lst.append(list(zip(names,price)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
